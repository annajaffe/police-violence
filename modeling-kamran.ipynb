{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, roc_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# !pip install prettytable\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basic Modeling Part 1a\n",
    "* Predicting race from police dept features (police demographics, policies) etc.\n",
    "* Database: victim_lemas_only1208.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of victims missing values: 0\n",
      "Response variable classes: ['A' 'B' 'H' 'N' 'O' 'W']\n"
     ]
    }
   ],
   "source": [
    "random_state = 109\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('data/victim_lemas_only1208.csv')\n",
    "response, multi_class = 'race', True\n",
    "total = len(data)\n",
    "\n",
    "# drop unnecessary columns\n",
    "cols_to_drop = ['date', 'city_wapo', 'state', 'AGENCYNAME', \n",
    "                'PERS_TOTAL', 'PERS_WHITE', 'PERS_BLACK', \n",
    "                'PERS_HISP', 'PERS_AMIND', 'PERS_ASIAN',\n",
    "                'PERS_HAWPI','PERS_MULTI','PERS_UNK']\n",
    "\n",
    "data = data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# drop nas\n",
    "data = data.dropna()\n",
    "print(f'Number of victims missing values: {total-len(data)}')\n",
    "\n",
    "# convert booleans to integers\n",
    "# bools_to_ints = ['signs_of_mental_illness', 'body_camera']\n",
    "# data[bools_to_ints] = data[bools_to_ints].astype(int)\n",
    "\n",
    "# convert response variable to 0 and n_classes-1\n",
    "if multi_class:\n",
    "    le = LabelEncoder()\n",
    "    data[response] = le.fit_transform(data[response])\n",
    "    print(f'Response variable classes: {le.classes_}')\n",
    "\n",
    "# convert other variables to dummies\n",
    "# data = pd.get_dummies(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 0.80\n",
      "Test size: 0.20\n"
     ]
    }
   ],
   "source": [
    "# splits\n",
    "train_size = 0.8\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(response, axis=1), data[response], train_size=train_size, random_state=random_state)\n",
    "\n",
    "col_labels = x_train.columns\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print(f'Train size: {len(x_train)/len(data):.2f}')\n",
    "print(f'Test size: {len(x_test)/len(data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive baseline accuracy: 0.498\n"
     ]
    }
   ],
   "source": [
    "baseline_train_score = accuracy_score(np.ones_like(y_train) * y_train.value_counts().idxmax(), y_train)\n",
    "print(f'Naive baseline accuracy: {baseline_train_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "class Model():\n",
    "    def __init__(self, model):\n",
    "        self.model = model.fit(x_train, y_train)\n",
    "    \n",
    "    def cv(self):\n",
    "        cv = cross_validate(self.model, x_train, y_train, cv=5, return_train_score=True, scoring='accuracy')\n",
    "        train_score = np.mean(abs(cv['train_score']))\n",
    "        val_score = np.mean(abs(cv['test_score']))\n",
    "        return train_score, val_score\n",
    "\n",
    "    def test_accuracy(self):\n",
    "        return self.model.score(x_test, y_test)\n",
    "    \n",
    "    def importance(self):\n",
    "        pi = permutation_importance(self.model, x_test, y_test, random_state=random_state)\n",
    "        \n",
    "        plt.figure(figsize=(5,5))\n",
    "        \n",
    "        sorted_idx = np.argsort(pi.importances_mean)[-10:]\n",
    "        indices = np.arange(0, len(pi.importances_mean[-10:])) + 0.5\n",
    "        \n",
    "        plt.barh(indices, pi.importances_mean[sorted_idx], height=0.7)\n",
    "        plt.yticks(indices, col_labels[sorted_idx])\n",
    "        plt.ylim((0, len(pi.importances_mean[-10:])))\n",
    "        plt.xlabel(\"Permutation Feature Importance\")\n",
    "        plt.title(f'{type(self.model).__name__}')\n",
    "\n",
    "\n",
    "knn = Model(KNeighborsClassifier())\n",
    "logit = Model(LogisticRegression(multi_class='ovr', max_iter=1000, random_state=random_state))\n",
    "logit_lasso = Model(LogisticRegression(multi_class='ovr', penalty='l1', solver='liblinear', max_iter=1000, random_state=random_state))\n",
    "dtree = Model(DecisionTreeClassifier(random_state=random_state))\n",
    "forest = Model(RandomForestClassifier(random_state=random_state))\n",
    "boost = Model(AdaBoostClassifier(random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for victim_lemas_only1208.csv\n",
      "+---------------------------+-------------------+------------------------+\n",
      "| Model                     | CV Train Accuracy | CV Validation Accuracy |\n",
      "+---------------------------+-------------------+------------------------+\n",
      "| KNN                       | 0.672             | 0.569                  |\n",
      "| Logistic Regression       | 0.593             | 0.586                  |\n",
      "| Lasso-Logistic Regression | 0.592             | 0.586                  |\n",
      "| Decision Tree             | 0.772             | 0.545                  |\n",
      "| Random Forest             | 0.772             | 0.597                  |\n",
      "| AdaBoost                  | 0.545             | 0.544                  |\n",
      "+---------------------------+-------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "knn_cv = knn.cv()\n",
    "logit_cv = logit.cv()\n",
    "logit_lasso = logit_lasso.cv()\n",
    "dtree_cv = dtree.cv()\n",
    "forest_cv = forest.cv()\n",
    "boost_cv = boost.cv()\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"CV Train Accuracy\", \"CV Validation Accuracy\"]\n",
    "x.add_rows(\n",
    "    [\n",
    "        [\"KNN\", knn_cv[0], knn_cv[1]],\n",
    "        [\"Logistic Regression\", logit_cv[0], logit_cv[1]],\n",
    "        [\"Lasso-Logistic Regression\", logit_lasso[0], logit_lasso[1]],\n",
    "        [\"Decision Tree\", dtree_cv[0], dtree_cv[1]],\n",
    "        [\"Random Forest\", forest_cv[0], forest_cv[1]],\n",
    "        [\"AdaBoost\", boost_cv[0], boost_cv[1]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x.align = \"l\"\n",
    "x.float_format = '.3'\n",
    "\n",
    "print('Accuracies for victim_lemas_only1208.csv')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Basic Modeling Part 1b\n",
    "* Predicting race from police dept features + WaPo features. If the victim demographics are better predictors, we can tie back to EDA\n",
    "* Database: victim_wapo_lemas_clean1208.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of victims missing values: 324\n",
      "Response variable classes: ['A' 'B' 'H' 'N' 'O' 'W']\n"
     ]
    }
   ],
   "source": [
    "random_state = 109\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('data/victim_wapo_lemas_clean1208.csv')\n",
    "response, multi_class = 'race', True\n",
    "total = len(data)\n",
    "\n",
    "\n",
    "# # drop unnecessary columns\n",
    "# wanted to keep race as single column, multi-class\n",
    "cols_to_drop = ['id', 'name', 'date', 'armed_wapo', 'gender', 'city_wapo', 'state', \n",
    "                'threat_level', 'longitude', 'latitude', 'is_geocoding_exact',\n",
    "               'street_address', 'zipcode', 'county', 'ORI_agency_id', \n",
    "                'ORI9','ORI9_match_type', 'race_A', 'race_B', \n",
    "                'race_H', 'race_N', 'race_O', 'race_W', 'AGENCYNAME',\n",
    "                'PERS_TOTAL', 'PERS_WHITE', 'PERS_BLACK', \n",
    "                'PERS_HISP', 'PERS_AMIND', 'PERS_ASIAN',\n",
    "                'PERS_HAWPI','PERS_MULTI','PERS_UNK'] + list(data.filter(regex='PERS'))\n",
    "\n",
    "data = data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# drop nas\n",
    "data = data.dropna()\n",
    "print(f'Number of victims missing values: {total-len(data)}')\n",
    "\n",
    "# convert booleans to integers\n",
    "# bools_to_ints = ['signs_of_mental_illness', 'body_camera']\n",
    "# data[bools_to_ints] = data[bools_to_ints].astype(int)\n",
    "\n",
    "# convert response variable to 0 and n_classes-1\n",
    "if multi_class:\n",
    "    le = LabelEncoder()\n",
    "    data[response] = le.fit_transform(data[response])\n",
    "    print(f'Response variable classes: {le.classes_}')\n",
    "\n",
    "# convert other variables to dummies\n",
    "data = pd.get_dummies(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 0.80\n",
      "Test size: 0.20\n"
     ]
    }
   ],
   "source": [
    "# splits\n",
    "train_size = 0.8\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(response, axis=1), data[response], train_size=train_size, random_state=random_state)\n",
    "col_labels = x_train.columns\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print(f'Train size: {len(x_train)/len(data):.2f}')\n",
    "print(f'Test size: {len(x_test)/len(data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive baseline accuracy: 0.503\n"
     ]
    }
   ],
   "source": [
    "baseline_train_score = accuracy_score(np.ones_like(y_train) * y_train.value_counts().idxmax(), y_train)\n",
    "print(f'Naive baseline accuracy: {baseline_train_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "knn = Model(KNeighborsClassifier())\n",
    "logit = Model(LogisticRegression(multi_class='ovr', max_iter=1000, random_state=random_state))\n",
    "logit_lasso = Model(LogisticRegression(multi_class='ovr', penalty='l1', solver='liblinear', max_iter=1000, random_state=random_state))\n",
    "dtree = Model(DecisionTreeClassifier(random_state=random_state))\n",
    "forest = Model(RandomForestClassifier(random_state=random_state))\n",
    "boost = Model(AdaBoostClassifier(random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = knn.cv()\n",
    "logit_cv = logit.cv()\n",
    "logit_lasso = logit_lasso.cv()\n",
    "dtree_cv = dtree.cv()\n",
    "forest_cv = forest.cv()\n",
    "boost_cv = boost.cv()\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"CV Train Accuracy\", \"CV Validation Accuracy\"]\n",
    "x.add_rows(\n",
    "    [\n",
    "        [\"KNN\", knn_cv[0], knn_cv[1]],\n",
    "        [\"Logistic Regression\", logit_cv[0], logit_cv[1]],\n",
    "        [\"Lasso-Logistic Regression\", logit_lasso[0], logit_lasso[1]],\n",
    "        [\"Decision Tree\", dtree_cv[0], dtree_cv[1]],\n",
    "        [\"Random Forest\", forest_cv[0], forest_cv[1]],\n",
    "        [\"AdaBoost\", boost_cv[0], boost_cv[1]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x.align = \"l\"\n",
    "x.float_format = '.3'\n",
    "\n",
    "print('Accuracies for victim_lemas_only1208.csv')\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
