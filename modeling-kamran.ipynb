{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# !pip install prettytable\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataframe into two Parts\n",
    "* Part 1: Predicting race from police dept and county features.\n",
    "* Part 2: Predicting race from police dept, county features and **WaPo features**. If the victim demographics are better predictors, we can tie back to EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vic_df = pd.read_csv('./data/victim_wapo_lemas_clean1208_with_county_demo.csv')\n",
    "\n",
    "all_col = ['id', 'name', 'date', 'armed_wapo', 'age', 'gender', 'race',\n",
    "       'city_wapo', 'state_x', 'signs_of_mental_illness', 'threat_level',\n",
    "       'flee', 'body_camera', 'longitude', 'latitude', 'is_geocoding_exact',\n",
    "       'street_address', 'zipcode', 'county_x', 'ORI_agency_id', 'ORI9',\n",
    "       'ORI9_match_type', 'is_male', 'is_tasered_and_shot', 'is_armed_gun',\n",
    "       'is_armed_knife', 'is_unarmed', 'is_armed_other', 'race_A', 'race_B',\n",
    "       'race_H', 'race_N', 'race_O', 'race_W', 'geo_Rural', 'geo_Suburban',\n",
    "       'geo_Undetermined', 'geo_Urban', 'threat_attack', 'threat_other',\n",
    "       'threat_undetermined', 'AGENCYNAME', 'POL_BWC', 'PERS_TRN_ACAD',\n",
    "       'PERS_TRN_FIELD', 'POL_COMP_EXTINV', 'POL_INV_INJRY', 'POL_INV_DTH',\n",
    "       'POL_INV_DCHG_GUN', 'PERS_CULTURE', 'PERS_CONFLICT', 'PERS_MALE',\n",
    "       'PERS_FEMALE', 'PERS_TOTAL', 'PERS_WHITE', 'FRAC_WHITE', 'PERS_BLACK',\n",
    "       'FRAC_BLACK', 'PERS_HISP', 'FRAC_HISP', 'PERS_AMIND', 'FRAC_AMIND',\n",
    "       'PERS_ASIAN', 'FRAC_ASIAN', 'PERS_HAWPI', 'FRAC_HAWPI', 'PERS_MULTI',\n",
    "       'FRAC_MULTI', 'PERS_UNK', 'FRAC_UNK', 'FIPS', 'state_y', 'county_y',\n",
    "       'TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black', 'Native',\n",
    "       'Asian', 'Pacific', 'Citizen', 'Income', 'IncomeErr', 'IncomePerCap',\n",
    "       'IncomePerCapErr', 'Poverty', 'ChildPoverty', 'Professional', 'Service',\n",
    "       'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n",
    "       'Walk', 'OtherTransp','WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n",
    "       'SelfEmployed', 'FamilyWork', 'Unemployment']\n",
    "\n",
    "norm100 = ['Hispanic', 'White', 'Black', 'Native',\n",
    "       'Asian', 'Pacific', 'Poverty', 'ChildPoverty', 'Professional', 'Service',\n",
    "       'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n",
    "       'Walk', 'OtherTransp','WorkAtHome', 'MeanCommute', 'PrivateWork', 'PublicWork',\n",
    "       'SelfEmployed', 'FamilyWork', 'Unemployment']\n",
    "\n",
    "vic_df.loc[:,norm100] = vic_df.loc[:,norm100]/100\n",
    "\n",
    "part1_cols = ['race','geo_Rural', 'geo_Suburban',\n",
    "       'geo_Undetermined', 'geo_Urban', 'POL_BWC', 'PERS_TRN_ACAD',\n",
    "       'PERS_TRN_FIELD', 'POL_COMP_EXTINV', 'POL_INV_INJRY', 'POL_INV_DTH',\n",
    "       'POL_INV_DCHG_GUN', 'PERS_CULTURE', 'PERS_CONFLICT', 'PERS_TOTAL', 'FRAC_WHITE',\n",
    "       'FRAC_BLACK', 'FRAC_HISP', 'FRAC_AMIND',\n",
    "        'FRAC_ASIAN', 'FRAC_HAWPI',\n",
    "       'FRAC_MULTI',\n",
    "       'Hispanic', 'White', 'Black', 'Native',\n",
    "       'Asian', 'Pacific', 'Income',\n",
    "        'Poverty', 'Unemployment']\n",
    "\n",
    "part2_cols = ['race','age',\n",
    "       'signs_of_mental_illness',\n",
    "       'flee', 'body_camera',\n",
    "       'is_male', 'is_tasered_and_shot', 'is_armed_gun',\n",
    "       'is_armed_knife', 'is_unarmed', 'is_armed_other', 'geo_Rural', 'geo_Suburban',\n",
    "       'geo_Undetermined', 'geo_Urban', 'threat_attack', 'threat_other',\n",
    "       'threat_undetermined', 'POL_BWC', 'PERS_TRN_ACAD',\n",
    "       'PERS_TRN_FIELD', 'POL_COMP_EXTINV', 'POL_INV_INJRY', 'POL_INV_DTH',\n",
    "       'POL_INV_DCHG_GUN', 'PERS_CULTURE', 'PERS_CONFLICT', 'PERS_TOTAL', 'FRAC_WHITE',\n",
    "       'FRAC_BLACK', 'FRAC_HISP', 'FRAC_AMIND',\n",
    "        'FRAC_ASIAN', 'FRAC_HAWPI',\n",
    "       'FRAC_MULTI',\n",
    "       'Hispanic', 'White', 'Black', 'Native',\n",
    "       'Asian', 'Pacific', 'Income',\n",
    "        'Poverty', 'Unemployment']\n",
    "\n",
    "vic_df.loc[vic_df['age'].isnull(),'age'] = vic_df.age.mean()\n",
    "vic_df.loc[vic_df['flee'].isnull(),'flee'] = 'Not fleeing'\n",
    "\n",
    "vic_df = vic_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Modeling Part 1a\n",
    "* Predicting race from police dept features (police demographics, policies) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response variable classes: ['A' 'B' 'H' 'N' 'O' 'W']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>geo_Rural</th>\n",
       "      <th>geo_Suburban</th>\n",
       "      <th>geo_Undetermined</th>\n",
       "      <th>geo_Urban</th>\n",
       "      <th>POL_BWC</th>\n",
       "      <th>PERS_TRN_ACAD</th>\n",
       "      <th>PERS_TRN_FIELD</th>\n",
       "      <th>POL_COMP_EXTINV</th>\n",
       "      <th>POL_INV_INJRY</th>\n",
       "      <th>...</th>\n",
       "      <th>FRAC_MULTI</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>Income</th>\n",
       "      <th>Poverty</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>720</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>50406.0</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>920</td>\n",
       "      <td>1640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.004</td>\n",
       "      <td>66754.0</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>960</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50657.0</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1304</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.004</td>\n",
       "      <td>81294.0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>840</td>\n",
       "      <td>720</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>60572.0</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44549.0</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4856</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>47801.0</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1114</td>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.001</td>\n",
       "      <td>54457.0</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1304</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.004</td>\n",
       "      <td>81294.0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49013.0</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4482 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      race  geo_Rural  geo_Suburban  geo_Undetermined  geo_Urban  POL_BWC  \\\n",
       "0        0          1             0                 0          0        0   \n",
       "1        5          0             1                 0          0        1   \n",
       "2        2          0             0                 0          1        1   \n",
       "3        5          0             0                 0          1        1   \n",
       "4        2          0             1                 0          0        1   \n",
       "...    ...        ...           ...               ...        ...      ...   \n",
       "4855     5          1             0                 0          0        0   \n",
       "4856     5          0             1                 0          0        0   \n",
       "4857     2          0             1                 0          0        1   \n",
       "4858     2          0             0                 0          1        1   \n",
       "4867     1          0             0                 0          1        1   \n",
       "\n",
       "      PERS_TRN_ACAD  PERS_TRN_FIELD  POL_COMP_EXTINV  POL_INV_INJRY  ...  \\\n",
       "0               720             480                0              1  ...   \n",
       "1               920            1640                0              0  ...   \n",
       "2               960             480                0              1  ...   \n",
       "3              1304             640                1              0  ...   \n",
       "4               840             720                0              1  ...   \n",
       "...             ...             ...              ...            ...  ...   \n",
       "4855              0             520                0              0  ...   \n",
       "4856            560             616                0              0  ...   \n",
       "4857           1114             720                1              0  ...   \n",
       "4858           1304             640                1              0  ...   \n",
       "4867            800             480                0              0  ...   \n",
       "\n",
       "      FRAC_MULTI  Hispanic  White  Black  Native  Asian  Pacific   Income  \\\n",
       "0            0.0     0.087  0.819  0.010   0.025  0.015    0.002  50406.0   \n",
       "1            0.0     0.162  0.682  0.018   0.004  0.092    0.004  66754.0   \n",
       "2            0.0     0.137  0.691  0.090   0.007  0.042    0.000  50657.0   \n",
       "3            0.0     0.153  0.412  0.053   0.002  0.335    0.004  81294.0   \n",
       "4            0.0     0.287  0.669  0.010   0.005  0.013    0.001  60572.0   \n",
       "...          ...       ...    ...    ...     ...    ...      ...      ...   \n",
       "4855         0.0     0.027  0.899  0.020   0.023  0.005    0.000  44549.0   \n",
       "4856         0.0     0.105  0.809  0.031   0.005  0.025    0.001  47801.0   \n",
       "4857         0.0     0.416  0.317  0.185   0.002  0.065    0.001  54457.0   \n",
       "4858         0.0     0.153  0.412  0.053   0.002  0.335    0.004  81294.0   \n",
       "4867         0.0     0.028  0.668  0.256   0.001  0.022    0.000  49013.0   \n",
       "\n",
       "      Poverty  Unemployment  \n",
       "0       0.172         0.129  \n",
       "1       0.118         0.076  \n",
       "2       0.151         0.069  \n",
       "3       0.132         0.068  \n",
       "4       0.129         0.066  \n",
       "...       ...           ...  \n",
       "4855    0.146         0.074  \n",
       "4856    0.161         0.050  \n",
       "4857    0.180         0.075  \n",
       "4858    0.132         0.068  \n",
       "4867    0.183         0.088  \n",
       "\n",
       "[4482 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 109\n",
    "\n",
    "# load data\n",
    "data = vic_df[part1_cols]\n",
    "response, multi_class = 'race', True\n",
    "total = len(data)\n",
    "\n",
    "# convert response variable to 0 and n_classes-1\n",
    "if multi_class:\n",
    "    le = LabelEncoder()\n",
    "    data_ = data.copy()\n",
    "    data_[response] = le.fit_transform(data.copy()[response])\n",
    "    print(f'Response variable classes: {le.classes_}')\n",
    "\n",
    "data = data_\n",
    "data = pd.get_dummies(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 0.80\n",
      "Test size: 0.20\n"
     ]
    }
   ],
   "source": [
    "# splits\n",
    "train_size = 0.8\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(response, axis=1), data[response], train_size=train_size, random_state=random_state)\n",
    "\n",
    "col_labels = x_train.columns\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print(f'Train size: {len(x_train)/len(data):.2f}')\n",
    "print(f'Test size: {len(x_test)/len(data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive baseline accuracy: 0.488\n"
     ]
    }
   ],
   "source": [
    "baseline_train_score = accuracy_score(np.ones_like(y_train) * y_train.value_counts().idxmax(), y_train)\n",
    "print(f'Naive baseline accuracy: {baseline_train_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "class Model():\n",
    "    def __init__(self, model):\n",
    "        self.model = model.fit(x_train, y_train)\n",
    "    \n",
    "    def cv(self):\n",
    "        cv = cross_validate(self.model, x_train, y_train, cv=5, return_train_score=True, scoring='accuracy')\n",
    "        train_score = np.mean(abs(cv['train_score']))\n",
    "        val_score = np.mean(abs(cv['test_score']))\n",
    "        return train_score, val_score\n",
    "\n",
    "    def test_accuracy(self):\n",
    "        return self.model.score(x_test, y_test)\n",
    "    \n",
    "    def importance(self):\n",
    "        pi = permutation_importance(self.model, x_test, y_test, random_state=random_state)\n",
    "        \n",
    "        plt.figure(figsize=(5,5))\n",
    "        \n",
    "        sorted_idx = np.argsort(pi.importances_mean)[-10:]\n",
    "        indices = np.arange(0, len(pi.importances_mean[-10:])) + 0.5\n",
    "        \n",
    "        plt.barh(indices, pi.importances_mean[sorted_idx], height=0.7)\n",
    "        plt.yticks(indices, col_labels[sorted_idx])\n",
    "        plt.ylim((0, len(pi.importances_mean[-10:])))\n",
    "        plt.xlabel(\"Permutation Feature Importance\")\n",
    "        plt.title(f'{type(self.model).__name__}')\n",
    "\n",
    "\n",
    "knn = Model(KNeighborsClassifier())\n",
    "logit = Model(LogisticRegression(multi_class='ovr', penalty='none', max_iter=1000, random_state=random_state))\n",
    "logit_lasso = Model(LogisticRegression(multi_class='ovr', penalty='l1', solver='liblinear', max_iter=1000, random_state=random_state))\n",
    "dtree = Model(DecisionTreeClassifier(max_depth=5, random_state=random_state))\n",
    "forest = Model(RandomForestClassifier(max_depth=5, random_state=random_state))\n",
    "boost = Model(AdaBoostClassifier(random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for victim_lemas_only1208.csv\n",
      "+-----------------------------+-------------------+------------------------+\n",
      "| Model                       | CV Train Accuracy | CV Validation Accuracy |\n",
      "+-----------------------------+-------------------+------------------------+\n",
      "| KNN                         | 0.695             | 0.596                  |\n",
      "| Logistic Regression         | 0.651             | 0.647                  |\n",
      "| Lasso-Logistic Regression   | 0.651             | 0.646                  |\n",
      "| Decision Tree (max_depth=5) | 0.671             | 0.637                  |\n",
      "| Random Forest (max_depth=5) | 0.686             | 0.655                  |\n",
      "| AdaBoost                    | 0.354             | 0.349                  |\n",
      "+-----------------------------+-------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "knn_cv = knn.cv()\n",
    "logit_cv = logit.cv()\n",
    "logit_lasso_cv = logit_lasso.cv()\n",
    "dtree_cv = dtree.cv()\n",
    "forest_cv = forest.cv()\n",
    "boost_cv = boost.cv()\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"CV Train Accuracy\", \"CV Validation Accuracy\"]\n",
    "x.add_rows(\n",
    "    [\n",
    "        [\"KNN\", knn_cv[0], knn_cv[1]],\n",
    "        [\"Logistic Regression\", logit_cv[0], logit_cv[1]],\n",
    "        [\"Lasso-Logistic Regression\", logit_lasso_cv[0], logit_lasso_cv[1]],\n",
    "        [\"Decision Tree (max_depth=5)\", dtree_cv[0], dtree_cv[1]],\n",
    "        [\"Random Forest (max_depth=5)\", forest_cv[0], forest_cv[1]],\n",
    "        [\"AdaBoost\", boost_cv[0], boost_cv[1]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x.align = \"l\"\n",
    "x.float_format = '.3'\n",
    "\n",
    "print('Accuracies for victim_lemas_only1208.csv')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Tuning Part 1b\n",
    "* Predicting race from police dept features (police demographics, policies) etc.\n",
    "* Database: victim_lemas_only1208.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4320 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  3.6min\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(random_state=random_state), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "print(f'Best RF estimator train accuracy: {best_grid.score(x_train, y_train)}')\n",
    "print(f'Best RF estimator test accuracy: {best_grid.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Basic Modeling Part 2\n",
    "* Predicting race from police dept features + WaPo features. If the victim demographics are better predictors, we can tie back to EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 109\n",
    "\n",
    "# load data\n",
    "data = vic_df[part2_cols]\n",
    "response, multi_class = 'race', True\n",
    "total = len(data)\n",
    "\n",
    "# convert response variable to 0 and n_classes-1\n",
    "if multi_class:\n",
    "    le = LabelEncoder()\n",
    "    data_ = data.copy()\n",
    "    data_[response] = le.fit_transform(data.copy()[response])\n",
    "    print(f'Response variable classes: {le.classes_}')\n",
    "\n",
    "data = data_\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits\n",
    "train_size = 0.8\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(response, axis=1), data[response], train_size=train_size, random_state=random_state)\n",
    "col_labels = x_train.columns\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print(f'Train size: {len(x_train)/len(data):.2f}')\n",
    "print(f'Test size: {len(x_test)/len(data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train_score = accuracy_score(np.ones_like(y_train) * y_train.value_counts().idxmax(), y_train)\n",
    "print(f'Naive baseline accuracy: {baseline_train_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "knn = Model(KNeighborsClassifier())\n",
    "logit = Model(LogisticRegression(multi_class='ovr', penalty='none', max_iter=1000, random_state=random_state))\n",
    "logit_lasso = Model(LogisticRegression(multi_class='ovr', penalty='l1', solver='liblinear', max_iter=1000, random_state=random_state))\n",
    "dtree = Model(DecisionTreeClassifier(max_depth=5, random_state=random_state))\n",
    "forest = Model(RandomForestClassifier(max_depth=5, random_state=random_state))\n",
    "boost = Model(AdaBoostClassifier(random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = knn.cv()\n",
    "logit_cv = logit.cv()\n",
    "logit_lasso_cv = logit_lasso.cv()\n",
    "dtree_cv = dtree.cv()\n",
    "forest_cv = forest.cv()\n",
    "boost_cv = boost.cv()\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"CV Train Accuracy\", \"CV Validation Accuracy\"]\n",
    "x.add_rows(\n",
    "    [\n",
    "        [\"KNN\", knn_cv[0], knn_cv[1]],\n",
    "        [\"Logistic Regression\", logit_cv[0], logit_cv[1]],\n",
    "        [\"Lasso-Logistic Regression\", logit_lasso_cv[0], logit_lasso_cv[1]],\n",
    "        [\"Decision Tree (max_depth=5)\", dtree_cv[0], dtree_cv[1]],\n",
    "        [\"Random Forest (max_depth=5)\", forest_cv[0], forest_cv[1]],\n",
    "        [\"AdaBoost\", boost_cv[0], boost_cv[1]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x.align = \"l\"\n",
    "x.float_format = '.3'\n",
    "\n",
    "print('Accuracies for victim_lemas_only1208.csv')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Tuning Part 2b\n",
    "* Predicting race from police dept features + WaPo features. If the victim demographics are better predictors, we can tie back to EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(random_state=random_state), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "print(f'Best RF estimator train accuracy: {best_grid.score(x_train, y_train)}')\n",
    "print(f'Best RF estimator test accuracy: {best_grid.score(x_test, y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
